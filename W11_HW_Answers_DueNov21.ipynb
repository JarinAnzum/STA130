{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee203cf7",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b2c6b",
   "metadata": {},
   "source": [
    "a) A Classification Decision Tree solves problems where the goal is to assign data into specific categories, like \"yes or no\" or \"group A or group B.\" It does this by asking a series of questions, such as \"Is age > 30?\" or \"Is income > 50K?\" until it reaches a decision. For example, it can be used to predict if an email is spam, if a patient has a disease, if a loan application should be approved, or if a transaction is fraudulent. It’s great for any situation where the outcome is a group or label.\n",
    "\n",
    "Examples of Real-World Applications:\n",
    "\n",
    "- Medical Diagnosis: Predicting whether a patient has a particular disease (e.g., \"diabetes\" or \"no diabetes\") based on medical test results.\n",
    "- Fraud Detection: Determining whether a transaction is fraudulent or not based on transactional features.\n",
    "- Spam Filtering: Classifying emails as \"spam\" or \"not spam\" based on email content and metadata.\n",
    "- Customer Segmentation: Categorizing customers into groups like \"high-value,\" \"medium-value,\" or \"low-value\" based on purchase history.\n",
    "- Loan Approval: Predicting whether a loan applicant is likely to default or repay the loan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb8290e",
   "metadata": {},
   "source": [
    "b) A Classification Decision Tree predicts categories by asking \"yes/no\" questions at each step, dividing the data into smaller groups until it lands on a final category (like \"approved\" or \"denied\").\n",
    "\n",
    "Multiple Linear Regression, on the other hand, predicts numbers by finding patterns between inputs and outputs. It uses a formula that adds up weighted values of the inputs (e.g., size of the house, number of bedrooms) to calculate a single number, like a house price or test score.\n",
    "\n",
    "The big difference is that decision trees predict groups or labels, while regression predicts specific numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5aa468",
   "metadata": {},
   "source": [
    "##### Link to ChatBot Session: https://chatgpt.com/share/673b8f24-855c-800f-a1d8-e7293f595026\n",
    "\n",
    "##### Summary of ChatBot Session: \n",
    "Our exchanges focused on understanding Classification Decision Trees and their applications, as well as comparing their predictive methods to Multiple Linear Regression. A Classification Decision Tree addresses problems requiring categorical predictions, such as identifying spam emails, predicting medical diagnoses, or determining loan approvals, by splitting data into groups through a series of yes/no questions. In contrast, Multiple Linear Regression predicts continuous numerical outcomes, like house prices or test scores, by finding patterns in data using a weighted formula. The key distinction lies in decision trees predicting categories, while regression predicts numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb2771e",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050abe1f",
   "metadata": {},
   "source": [
    "Here are examples of real-world scenarios where each metric may be most appropriate:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e679eb",
   "metadata": {},
   "source": [
    "1. Accuracy:\n",
    "- Scenario: Assessing the overall performance of a spam email filter.\n",
    "- Rationale: Accuracy is suitable when both false positives (non-spam marked as spam) and false negatives (spam missed) are equally problematic. It provides a broad measure of the filter's success in correctly categorizing emails.\n",
    "\n",
    "2. Sensitivity:\n",
    "- Scenario: Screening for a rare disease in medical diagnostics.\n",
    "- Rationale: Sensitivity is critical when identifying all actual positives (e.g., diseased patients) is more important than avoiding false alarms, as missing a positive case could have severe consequences.\n",
    "\n",
    "3. Specificity:\n",
    "- Scenario: Testing for drug use in a workplace setting.\n",
    "- Rationale: Specificity is key when the focus is on minimizing false positives (e.g., wrongly accusing someone of drug use), ensuring innocent individuals are not incorrectly flagged.\n",
    "\n",
    "4. Precision:\n",
    "- Scenario: Detecting fraudulent transactions in banking.\n",
    "- Rationale: Precision is crucial when the cost of false positives (flagging legitimate transactions as fraud) is high, as it ensures that flagged cases are highly likely to be actual frauds, reducing unnecessary disruptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca0a255",
   "metadata": {},
   "source": [
    "##### Link to ChatBot Session: https://chatgpt.com/share/673b8f24-855c-800f-a1d8-e7293f595026\n",
    "\n",
    "##### Summary of ChatBot Session: \n",
    "Our discussion explored real-world applications for evaluating the performance of classification metrics: accuracy, sensitivity, specificity, and precision. Accuracy is ideal for balanced scenarios like spam email filtering, where both false positives and false negatives are equally important. Sensitivity is crucial for identifying true positives, such as detecting rare diseases in medical diagnostics, where missing cases can have severe consequences. Specificity is essential for minimizing false positives in scenarios like workplace drug testing to avoid unfair accusations. Precision is valuable in applications like fraud detection, where the cost of false positives is high, ensuring flagged cases are likely correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e512d84d",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6187e57b",
   "metadata": {},
   "source": [
    "To perform initial EDA and summarization of the Amazon books dataset, we first preprocess it to ensure the data meets the required specifications. This involves removing irrelevant columns, handling missing data, and converting data types as specified.\n",
    "\n",
    "The dataset contains columns (`Weight_oz`, `Width`, and `Height`) that are not relevant to our analysis. Removing these columns reduces dimensionality and avoids including irrelevant features in our exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e6ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "ab_reduced = ab.drop(columns=['Weight_oz', 'Width', 'Height'])\n",
    "ab_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b698fd95",
   "metadata": {},
   "source": [
    "Rows containing NaN entries can cause issues when performing machine learning and statistical operations. To address this, we drop rows with missing values from the dataset after subsetting the columns of interest. This ensures no unnecessary data loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2329ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values\n",
    "ab_reduced_noNaN = ab_reduced.dropna()\n",
    "ab_reduced_noNaN.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a5e2b",
   "metadata": {},
   "source": [
    "Data types in a dataset must align with the nature of the data they represent. To meet the requirements:\n",
    "\n",
    "- Convert `Pub year` and `NumPages` to integers because they represent numerical values.\n",
    "- Convert `Hard_or_Paper` to a categorical type as it represents a qualitative attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae399b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types\n",
    "ab_reduced_noNaN['Pub year'] = ab_reduced_noNaN['Pub year'].astype(int)\n",
    "ab_reduced_noNaN['NumPages'] = ab_reduced_noNaN['NumPages'].astype(int)\n",
    "ab_reduced_noNaN['Hard_or_Paper'] = ab_reduced_noNaN['Hard_or_Paper'].astype('category')\n",
    "ab_reduced_noNaN.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f5bb20",
   "metadata": {},
   "source": [
    "Now that the dataset is clean, we can perform some initial summarization:\n",
    "- Viewing basic statistics for numerical columns.\n",
    "- Checking the distribution of categorical variables.\n",
    "- Confirming the dataset’s shape to understand how much data remains after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca00009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize numerical columns\n",
    "ab_reduced_noNaN.describe()\n",
    "\n",
    "# Summarize categorical column\n",
    "ab_reduced_noNaN['Hard_or_Paper'].value_counts()\n",
    "\n",
    "# Dataset shape\n",
    "ab_reduced_noNaN.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cc6d03",
   "metadata": {},
   "source": [
    "This streamlined, processed dataset can now be used for even more analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bc3e89",
   "metadata": {},
   "source": [
    "##### Link to ChatBot Session: https://chatgpt.com/share/673b9144-77c0-800f-9791-42003b7a19e7\n",
    "\n",
    "##### Summary of ChatBot Session: \n",
    "We discussed how to preprocess the Amazon books dataset to prepare it for exploratory data analysis by addressing specific requirements. The steps included removing the irrelevant columns Weight_oz, Width, and Height, dropping rows with missing values (NaN) to ensure a clean dataset, and converting the columns Pub year and NumPages to integer types, while setting Hard_or_Paper as a categorical type. Following these preprocessing steps, we summarized the dataset by exploring its numerical statistics, the distribution of categorical variables, and its overall shape to confirm it was ready for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a162e0e9",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c58b7f",
   "metadata": {},
   "source": [
    "To prepare the dataset for training and testing, we split the data into an 80% training set (`ab_reduced_noNaN_train`) and a 20% testing set (`ab_reduced_noNaN_test`). This split helps validate the model by training on one subset and testing its performance on unseen data.\n",
    "\n",
    "Using a reproducible random seed ensures consistency in the split, allowing for the same results when rerunning the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed28c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform 80/20 split with a random seed\n",
    "ab_reduced_noNaN_train, ab_reduced_noNaN_test = train_test_split(\n",
    "    ab_reduced_noNaN, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Report the number of observations in each set\n",
    "len_train = len(ab_reduced_noNaN_train)\n",
    "len_test = len(ab_reduced_noNaN_test)\n",
    "\n",
    "len_train, len_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9733ae06",
   "metadata": {},
   "source": [
    "We use the training set (`ab_reduced_noNaN_train`) to fit the model because it allows the classifier to learn patterns from the majority of the dataset. The testing set remains unseen during training, ensuring an unbiased evaluation of the model’s generalization ability. Fitting the model on the test set would overfit to that specific data, invalidating its purpose as a measure of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2188d316",
   "metadata": {},
   "source": [
    "A DecisionTreeClassifier is a supervised learning model that splits data based on features to predict a target variable. The .fit() method is used to train the model by identifying the best thresholds for splitting the data into subsets that minimize classification errors. \n",
    "\n",
    "By setting the `max_depth` parameter to 2, the model creates a simple tree with at most two levels of splits, ensuring interpretability and reducing the risk of overfitting. This makes the model suitable for a straightforward analysis of how List Price influences book type classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f875d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and feature variables\n",
    "y = pd.get_dummies(ab_reduced_noNaN_train[\"Hard_or_Paper\"])['H']\n",
    "X = ab_reduced_noNaN_train[['List Price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b15d38",
   "metadata": {},
   "source": [
    "To prepare the data for fitting the tree, two key steps are performed. \n",
    "1. The target variable (y) is created by using one-hot encoding to represent \"Hardcover\" as 1 and \"Paperback\" as 0. This binary variable serves as the dependent variable for the classification. \n",
    "2. The feature matrix (X) is defined by selecting List Price as the sole predictor. This isolates the relationship between a book's price and its type for analysis.\n",
    "\n",
    "We use the DecisionTreeClassifier with a maximum depth of 2 to predict book types based on their price. The .fit() method trains the model using the training data, and tree.plot_tree() shows the splits the model makes. This gives a clear visual of how price ranges predict whether a book is hardcover or paperback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c264a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DecisionTreeClassifier\n",
    "clf = tree.DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Visualize the decision tree\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 8))\n",
    "tree.plot_tree(clf, feature_names=['List Price'], class_names=['Paperback', 'Hardcover'], filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7d4d1e",
   "metadata": {},
   "source": [
    "The decision tree shows how List Price affects predictions. For example, if the price is below a certain point, the model predicts \"Paperback.\" If it’s higher, the model refines the prediction further, possibly deciding \"Hardcover.\" The tree explains which price ranges lead to which predictions and the confidence in each choice, making it easy to see how the model works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26c5b4a",
   "metadata": {},
   "source": [
    "##### Link to ChatBot Session: https://chatgpt.com/share/673b9144-77c0-800f-9791-42003b7a19e7\n",
    "\n",
    "##### Summary of ChatBot Session: \n",
    "We discussed splitting the Amazon books dataset into an 80% training set and a 20% testing set using train_test_split, ensuring reproducibility with a random seed. I explained why the training data is used to fit models to avoid overfitting the test set. We then focused on training a DecisionTreeClassifier to predict whether a book is a hardcover or paperback based on its List Price, using a maximum tree depth of 2 for simplicity. I provided an explanation of key steps like defining target (y) and feature (X) variables, fitting the model, and visualizing the decision tree to show how price influences predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82452187",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba853d3e",
   "metadata": {},
   "source": [
    "We can extend the decision tree model to include three predictors: `NumPages`, Thick, and List Price. We use the same training and testing datasets from the previous problem and set the `max_depth` of the tree to 4 for increased complexity. This allows the model to capture more nuanced patterns in the data while predicting whether a book is a hardcover or paperback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be307545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new feature matrix X\n",
    "X = ab_reduced_noNaN_train[['NumPages', 'Thick', 'List Price']]\n",
    "y = pd.get_dummies(ab_reduced_noNaN_train[\"Hard_or_Paper\"])['H']\n",
    "\n",
    "# Train a new DecisionTreeClassifier with max_depth=4\n",
    "clf2 = tree.DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "clf2.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e8bad",
   "metadata": {},
   "source": [
    "The decision tree trained on multiple predictors is more complex, so visualizing it helps us understand its decision-making process. Using tree.plot_tree provides a quick overview, while graphviz creates a more detailed and readable visualization. This visualization illustrates how the model splits data based on combinations of `NumPages`, `Thick`, and `List Price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7252ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tree using tree.plot_tree\n",
    "plt.figure(figsize=(16, 10))\n",
    "tree.plot_tree(\n",
    "    clf2,\n",
    "    feature_names=['NumPages', 'Thick', 'List Price'],\n",
    "    class_names=['Paperback', 'Hardcover'],\n",
    "    filled=True\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9010410c",
   "metadata": {},
   "source": [
    "The decision tree makes **predictions** for `clf2` by splitting the data at thresholds for the predictors (NumPages, Thick, and List Price). At each node, the tree asks a \"yes or no\" question (e.g., \"Is List Price > 20?\"). Depending on the answer, the model moves to the next branch, narrowing down the possibilities. After at most four levels of splits (due to max_depth=4), the tree reaches a leaf node with a prediction and a probability for each class. For example, books with fewer pages, low thickness, and a lower price might be classified as paperbacks, while books with higher values for these predictors might be classified as hardcovers.\n",
    "\n",
    "Using multiple predictors allows `clf2` to combine information from all three features, improving its ability to differentiate between hardcovers and paperbacks. However, the added complexity might make the model less interpretable compared to a simpler tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217a81f2",
   "metadata": {},
   "source": [
    "##### Link to ChatBot Session: https://chatgpt.com/share/673b9144-77c0-800f-9791-42003b7a19e7\n",
    "\n",
    "##### Summary of ChatBot Session: \n",
    "We discussed training a new classification decision tree (clf2) using the same training and testing datasets, but this time with three predictors: NumPages, Thick, and List Price. The tree was trained with a maximum depth of 4 to allow for more detailed splits. I provided guidance on setting up the feature matrix and target variable, training the model, and visualizing it using tree.plot_tree. We explained how the tree makes predictions by splitting data at specific thresholds for the predictors, ultimately classifying books as either hardcover or paperback. The inclusion of multiple features enables clf2 to capture more complex patterns while maintaining interpretability through visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a162d4",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b2994c",
   "metadata": {},
   "source": [
    "To evaluate the performance of the classifiers (clf and clf2), we create confusion matrices using the test set (ab_reduced_noNaN_test). From these matrices, we calculate sensitivity, specificity, and accuracy. Here's how these metrics are defined:\n",
    "\n",
    "Sensitivity (Recall): The proportion of actual positives correctly identified (TP / (TP + FN)).\n",
    "Specificity: The proportion of actual negatives correctly identified (TN / (TN + FP)).\n",
    "Accuracy: The proportion of all predictions that are correct ((TP + TN) / Total)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f03aba",
   "metadata": {},
   "source": [
    "1. Prepare the Test Data: First, we extract the features (X) and target variable (y) from the test set for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d0582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "y_test = pd.get_dummies(ab_reduced_noNaN_test[\"Hard_or_Paper\"])['H']\n",
    "\n",
    "# For clf (List Price only)\n",
    "X_test_clf = ab_reduced_noNaN_test[['List Price']]\n",
    "y_pred_clf = clf.predict(X_test_clf)\n",
    "\n",
    "# For clf2 (NumPages, Thick, List Price)\n",
    "X_test_clf2 = ab_reduced_noNaN_test[['NumPages', 'Thick', 'List Price']]\n",
    "y_pred_clf2 = clf2.predict(X_test_clf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9032932b",
   "metadata": {},
   "source": [
    "2. Create Confusion Matrices: We calculate confusion matrices for clf and clf2 using `confusion_matrix`. The matrix provides true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN), which are then used to calculate the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd7472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Confusion matrices\n",
    "cm_clf = confusion_matrix(y_test, y_pred_clf)\n",
    "cm_clf2 = confusion_matrix(y_test, y_pred_clf2)\n",
    "\n",
    "# Display confusion matrices\n",
    "cm_clf, cm_clf2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc7a0a",
   "metadata": {},
   "source": [
    "3. Calculate Metric for Each Model: Using the confusion matrices, we calculate sensitivity, specificity, and accuracy for both models. We use np.round() to format the results to three significant digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980edd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate metrics\n",
    "def calculate_metrics(cm):\n",
    "    TP = cm[1, 1]\n",
    "    TN = cm[0, 0]\n",
    "    FP = cm[0, 1]\n",
    "    FN = cm[1, 0]\n",
    "    \n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    accuracy = (TP + TN) / cm.sum()\n",
    "    \n",
    "    return np.round(sensitivity, 3), np.round(specificity, 3), np.round(accuracy, 3)\n",
    "\n",
    "# Metrics for clf\n",
    "sensitivity_clf, specificity_clf, accuracy_clf = calculate_metrics(cm_clf)\n",
    "\n",
    "# Metrics for clf2\n",
    "sensitivity_clf2, specificity_clf2, accuracy_clf2 = calculate_metrics(cm_clf2)\n",
    "\n",
    "# Display results\n",
    "{\n",
    "    \"clf\": {\"sensitivity\": sensitivity_clf, \"specificity\": specificity_clf, \"accuracy\": accuracy_clf},\n",
    "    \"clf2\": {\"sensitivity\": sensitivity_clf2, \"specificity\": specificity_clf2, \"accuracy\": accuracy_clf2}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d42fe",
   "metadata": {},
   "source": [
    "Here is how we can interpret the results:-\n",
    "\n",
    "For `clf` (List Price only):\n",
    "- Sensitivity indicates how well the model identifies hardcovers correctly.\n",
    "- Specificity shows how well it avoids misclassifying paperbacks.\n",
    "- Accuracy reflects overall prediction correctness.\n",
    "\n",
    "For `clf2` (NumPages, Thick, List Price):\n",
    "- Both sensitivity and specificity account for more predictors, likely improving overall performance.\n",
    "- Accuracy may improve due to better handling of complex patterns.\n",
    "\n",
    "These metrics provide a clear understanding of the strengths and weaknesses of each model based on the test data. Let me know if you'd like me to visualize the results further!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5a6820",
   "metadata": {},
   "source": [
    "##### Link to ChatBot Session: https://chatgpt.com/share/673b9144-77c0-800f-9791-42003b7a19e7\n",
    "\n",
    "##### Summary of ChatBot Session: \n",
    "We discussed evaluating the performance of two classification models, clf (using List Price) and clf2 (using NumPages, Thick, and List Price), by generating confusion matrices from the test set and calculating sensitivity, specificity, and accuracy for each. I explained how these metrics reflect the models' ability to classify books as hardcover or paperback, highlighting how clf2 likely performs better due to the inclusion of additional predictors, enabling it to handle more complex patterns. We also emphasized the importance of specificity in measuring how well the models avoid misclassifying paperbacks, alongside overall prediction accuracy and sensitivity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff260a8b",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68a212e",
   "metadata": {},
   "source": [
    "The difference between the two confusion matrices arises from the features used for predictions. The first matrix relies solely on `List Price`, while the second incorporates `NumPages` and `Thick`, providing more contextual information. Including additional features allows the model to capture more complex patterns, potentially reducing misclassifications. The confusion matrices for `clf` and `clf2` are better because they likely use more robust models or optimized feature sets, leading to improved classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bce2a58",
   "metadata": {},
   "source": [
    "##### Link to ChatBot Session: https://chatgpt.com/c/673bfcd8-1b40-800f-88ea-cbb54d68419e\n",
    "\n",
    "##### Summary of ChatBot Session: \n",
    "Our exchanges focused on analyzing differences between two confusion matrices generated using different feature sets for a classification task. The first confusion matrix relied on a single feature (List Price), while the second used additional features (NumPages and Thick) to improve prediction accuracy by capturing more complex relationships in the data. We also discussed why the confusion matrices for clf and clf2 were better, emphasizing their likely use of more robust models or optimized features. The analysis highlights the importance of feature selection and model optimization in improving classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1113db",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137a5a4c",
   "metadata": {},
   "source": [
    "Feature importances are like a scorecard for the questions (splits) the decision tree asked to make predictions. Each question is about a predictor (feature), and the tree gives higher scores to the features that helped improve predictions the most. These scores are stored in the `.feature_importances_` attribute of the decision tree. To know which feature each score belongs to, we look at the `.feature_names_in_ attribute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91306ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve feature importances and corresponding feature names\n",
    "importances = clf2.feature_importances_\n",
    "feature_names = clf2.feature_names_in_\n",
    "\n",
    "# Display feature importances and names\n",
    "importances, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e45e8bd",
   "metadata": {},
   "source": [
    "To find the most important feature, we look for the feature with the highest score in `.feature_importances_`. This is like finding the student with the highest grade in a class. We then use `.feature_names_in_` to find the name of that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9bedad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the most important feature\n",
    "most_important_index = importances.argmax()\n",
    "most_important_feature = feature_names[most_important_index]\n",
    "\n",
    "# Display the most important feature\n",
    "most_important_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db017b1",
   "metadata": {},
   "source": [
    "To understand how all the features contributed to the predictions, we create a bar chart. This is like creating a leaderboard where each feature’s score is shown as a bar. The taller the bar, the more important the feature was for the decision tree’s predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b67f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names, importances)\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importances for clf2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673e4c45",
   "metadata": {},
   "source": [
    "##### Link to ChatBot Session: https://chatgpt.com/c/673c0090-aacc-800f-b5e6-24d27470cca7\n",
    "\n",
    "##### Summary of ChatBot Session: \n",
    "In our exchanges, we discussed how to analyze and visualize feature importances for a scikit-learn classification decision tree model (clf2). The process involved extracting feature importance scores using the .feature_importances_ attribute and mapping these scores to predictor variable names using .feature_names_in_. We identified the most important feature by finding the highest importance score and its corresponding name, then visualized the relative contributions of all features using a bar chart. Each step included clear explanations and Python code, ensuring both understanding and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f86356",
   "metadata": {},
   "source": [
    "### Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a510f1e9",
   "metadata": {},
   "source": [
    "Linear regression coefficients quantify the precise change in the outcome for a one-unit change in a predictor, assuming other variables are held constant. \n",
    "\n",
    "In decision trees, feature importances reflect a feature's overall contribution to reducing impurity across all splits, capturing its relative influence without assuming linearity or controlling for other variables. \n",
    "\n",
    "This makes regression coefficients more precise, while feature importances offer broader interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c256f75",
   "metadata": {},
   "source": [
    "##### ChatBot Not Used for This Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f42801",
   "metadata": {},
   "source": [
    "### Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a03658",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Yes!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a798d846",
   "metadata": {},
   "source": [
    "All summaries were retrieved with: \"Please provide a summary of our exchanges here so I can submit them as a record of our interactions as part of a homework assignment\" ^_^"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
